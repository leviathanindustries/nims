
<div class="strap strap-compact">
  <div class="jumbotron" style="background-color:#66bbff;">
    <div class="container-fluid">
      <div class="row">
        <div class="col-md-12">
          <h1 style="color:white;font-size:54px;text-align:center;">NIMS experimental UI investigation</h1>
        </div>
      </div>
    </div>
  </div>
</div>

<style>
  h2 {
    margin-top:60px;
  }
  h3 {
    margin-top:40px;
  }
  h4 {
    margin-top:30px;
  }
</style>

<div class="strap strap-compact">
  <div class="container-fluid" style="max-width:1000px;">
    <div class="row">
      <div class="col-md-3" style="position:sticky;top:60px;">
        <div class="well" style="padding:0px;">
          <ul class="nav" style="margin:0px;padding:0px;">
            <li style="border-bottom:1px solid #ddd;"><a style="padding:4px 5px 1px 15px;" href="/">Introduction</a></li>
            <li style="border-bottom:1px solid #ddd;"><a style="padding:4px 5px 1px 15px;" href="#data">Data</a></li>

            <li><a style="padding:4px 5px 1px 15px;" href="#methods">Methods</a></li>
            <li><a style="padding:2px 5px 1px 35px;" href="#indexing">Full text indexing</a></li>
            <li><a style="padding:2px 5px 1px 35px;" href="#tdm">Text and Data mining</a></li>
            <li><a style="padding:2px 5px 1px 35px;" href="#nlp">Natural language processing</a></li>
            <li><a style="padding:2px 5px 1px 35px;" href="#language">Language translation</a></li>
            <li><a style="padding:2px 5px 1px 35px;" href="#ml">Machine learning</a></li>
            <li style="border-bottom:1px solid #ddd;"><a style="padding:2px 5px 1px 35px;" href="#phash">Perceptual hashing</a></li>

            <li><a style="padding:4px 5px 1px 15px;" href="#demos">Demonstrations</a></li>

            <li><a style="padding:2px 5px 1px 35px;" href="#dataset">Dataset</a></li>
            <!--<li><a style="padding:2px 5px 1px 55px;" href="#dataset-csv">Dataset CSV</a></li>
            <li style="border-bottom:1px solid #ddd;"><a style="padding:2px 5px 1px 55px;" href="#dataset-graphs">Dataset graphs</a></li>-->

            <li><a style="padding:2px 5px 1px 35px;" href="#imeji">Imeji</a></li>
            <!--<li><a style="padding:2px 5px 1px 55px;" href="#imeji-tiles">Imeji tiles</a></li>
            <li style="border-bottom:1px solid #ddd;"><a style="padding:2px 5px 1px 55px;" href="#imeji-analysis">Imeji analysis</a></li>-->

            <li><a style="padding:2px 5px 1px 35px;" href="#pdf">PDF</a></li>
            <!--<li style="border-bottom:1px solid #ddd;"><a style="padding:2px 5px 1px 55px;" href="#pdf-entities">PDF entities</a></li>-->

            <li><a style="padding:2px 5px 1px 35px;" href="#pubman">Pubman</a></li>
            <!--<li style="border-bottom:1px solid #ddd;"><a style="padding:2px 5px 1px 55px;" href="#pubman-graph">Pubman graphs</a></li>-->

            <li><a style="padding:2px 5px 1px 35px;" href="#samurai">Samurai</a></li>
            <!--<li><a style="padding:2px 5px 1px 55px;" href="#samurai-network">Samurai network</a></li>
            <li style="border-bottom:1px solid #ddd;"><a style="padding:2px 5px 1px 55px;" href="#samurai-graphs">Samurai graphs</a></li>-->

            <li style="border-bottom:1px solid #ddd;"><a style="padding:2px 5px 1px 35px;" href="#search">Search / suggest</a></li>

            <li><a style="padding:4px 5px 1px 15px;" href="#future">Future work</a></li>
          </ul>
        </div>
      </div>

      <div class="col-md-7" style="text-align:justify;">

        <p>When data is collected from various different sources it allows us to
        explore relations between the datasets that would not have been possible from within just one of them.
        Also, we can explore it with different intentions to the original purpose. And of course, by collecting
        and storing the data we can ensure it remains available long term, and curate it to make it easier for
        others to explore and use. So what new and interesting things can we do with this resource?</p>

        <h3>Maintaining data over time</h3>

        <p>Hyrax is a good platform for receiving and storing research articles, data, and metadata. It also allows
        for curation, to ensure high quality metadata standards. Over time, this should
        provide an increasingly high quality source of data and metadata.</p>

        <p>We also already have vocabulary lists, providing terms that researchers
        may be expected to use to describe their submissions. These can be used to improve curation and searchability
        of the submissions, and can also be used for keyword analysis and entity extraction.</p>

        <p>We would also want to export data, in batches to meet certain requirements,
        that could be created by queries of the Hyrax storage layer. With a suitable export mechanism and/or API,
        it would be possible to pipeline data into downstream TDM (and other) activities.</p>

        <h3>Linking and enabling new discovery</h3>

        <p>By analysing incoming data documents to generate metadata suggestions,
        we can help researchers submit their data and help curators to maintain it.</p>

        <p>Finding keywords and entities within the data, we can link the data
        to other sources to provide more information and context, such as Wikipedia, Wikidata, Google
        Knowledge Graph, and so on.</p>

        <p>We can also expose previously unrecognised values and relations. For example, we can find entities not yet in our
        vocabularies, analyse images to learn more about them, visualise relations between entities, or develop a system to detect
        unusual results within submitted data.</p>



        <h2>Data</h2>

        <p>So far we have worked with four kinds of data.</p>

        <ul>
          <li>Sample datasets (various XML, CSV, text, data, image files in ZIP)</li>
          <li>Imeji metadata (XML)</li>
          <li>Pubman metadata (XML)</li>
          <li>Pubman articles (PDF)</li>
        </ul>

        <p>We also have NIMS vocabulary files for use in Hyrax,
        and I knew of the Samurai Researchers Directory Service API, so I used that as well:</p>

        <ul>
          <li>NIMS vocabulary files</li>
          <li>Samurai metadata (JSON)</li>
        </ul>

        <h3>Getting data from Hyrax</h3>

        <p>Hyrax has a search endpoint, but by default it is restricted in what it delivers, althouth it can be configured
        and can provide JSON (by URL param format=json). It does not have a built-in export function or full
        API - there are some as add-ons, but to quite specific purposes.
        A Hyrax working group is currently reviewing export and API requirements, but it is in early stages.</p>

        <p>Due to these reasons and the other constraints on the project team to achieve the main requirements, I chose
        to process the data in a separate system, and to import the sample data from source rather than from Hyrax.</p>

        <h3>Processing XML</h3>

        <p>The sample XML metadata format sometimes contains elements within elements in one file, whereas
          in other files those same parent elements contain strings. Such elements in the pubman metadata
          include abstract, alternative, and subject. This structure makes it harder to relate entities across
          metadata types.</p>

        <p>The schema structure, both in the XML metadata and the Samurai JSON metadata, uses nesting
        methods that do not necessarily put the same entities in the same elements, or in the same places
        within element trees.</p>

        <p>Metadata schemas and structures must to some extent be dictated by the preferences of the
        researchers submitting the data, and also by the requirements of Hyrax. So it may be that it will remain a better
        approach to export data from Hyrax for TDM work elsewhere, in which case we should focus on making data export a
        priority in future work.</p>

        <p><a target="_blank" href="https://api.nims.test.cottagelabs.com/xml">View a sample dataset XML metadat file</a></p>
        <p><a target="_blank" href="https://api.nims.test.cottagelabs.com/dataset?q=%22AES-narrow%22&exclude=csv">View same file after import</a></p>

        <p>As I had decided to import all the data to an alternative system, I also converted all the metadata to
        JSON and to flatten structures in some places, as it is easier to process and visualise that way. In general, a
        document-oriented approach is better for analysis because a relational approach requires dereferencing and
        flattening anyway. The presence of a relevant piece of text is often as useful as a reference to further metadata.</p>


        <h3>Processing JSON</h3>

        <p>I retrieved Samurai metadata by iterating queries to the Samurai JSON API until I had 832 records.
        The API worked well, and the structure of the Samurai data is easier to use, although it does
        have a particular oddity of using objects with numbers for keys instead of just using lists. Presumably this is to
        overcome the fact that lists do not guarantee their order (although I have never actually seen one come out in a
        different order in real world usage). But it makes it more difficult to analyse the data as essentially every number
        key acts as another object type in the tree, meaning for example that the first author in an author list is in
        a parent object name "0", while the next is in "1", and so on. So these were flattened to normal lists.</p>

        <p><a target="_blank" href="https://samurai.nims.go.jp/profiles/kurashima_keiji.json">View a Samurai record with object with numbered keys</a></p>
        <p><a target="_blank" href="https://api.nims.test.cottagelabs.com/samurai?q=id:\"9d61a3c8-fb55-48ca-b083-27b03ca8a80f\"">View the same record after import</a></p>


        <h3>Processing CSV</h3>

        <p>Although the sample datasets contain raw experimental result data within CSV files in some cases,
        that CSV data does not necessarily conform to a standard CSV structure. To make better use of CSV files
        in future, it would be good for them to have the following structure:</p>

        <ul>
          <li>The sample files include comments in lines with a # - this is not technically allowed in CSV, but is
          a suitable way to provide metadata, and has been extracted in the above demonstrations. Future CSV datasets
          would need to adhere to this approach, if it is hoped to be used reliably in future work.</li>
          <li>Ensure that data is arranged across rows, with column headers across a row as well. For example one sample dataset
          aligned the headers downwards in row 1, with the data downwards in row 2 - this breaks the expected
          data layout and is not possible to identify this difference so this CSV does not successfully parse.</li>
          <li>Put column headers in the row preceding the data, ensuring there is a header for each row. Whilst the
          comments in the sample files do sometimes include key names for #dimension, #x, #y, etc, they do not do this
          in a way that reliably relates to the amount of rows in the data.</li>
          <li>Ensure the data is separated out into rows and columns. One Sample dataset included all data points as one
          comma-separated list in one row, making it impossible to try to relate the data to the commented metadata
          in the file.</li>
        </ul>

        <p>Examples can be found in the project repository, in 2019-01-31 sample dataset characterization samples. The most useful ones
        were titled primary.csv or id.csv, although there were others such as DataBase01.104.csv in the XPS_survey sample,
        HAADF.csv in the STEM sample, and MIDATA001.107.csv in the XPS_narrow sample. The Database01.104.csv file was usable
        as it had the same format as the id.csv file, but HAADF.csv had same comment style but all content in a single line,
        and MIDATA001.107.csv had 7 data columns but no way to identify column headers for more than two columns.</p>

        <p><a target="_blank" href="https://api.nims.test.cottagelabs.com/look/csv?raw=true">View a sample CSV file from one of the datasets</a></p>
        <p><a target="_blank" href="https://api.nims.test.cottagelabs.com/dataset?q=%22AES-survey%22&include=csv">View the same file as JSON in an imported dataset</a></p>


        <h3>Processing images</h3>

        <p>Images linked in metadata such as in Imeji or Samurai were not imported en masse, but were retrieved and
        analysed when necessary.</p>

        <p><a target="_blank" href="https://api.nims.test.cottagelabs.com/imeji?q=*&size=1">View an imported Imeji record with an image link</a></p>


        <h3>Processing PDF and text</h3>

        <p>A subset of 500 Pubman PDFs were parsed to extract the text.
        No structure was assumed. Images in PDFs
        could also be extracted, but this has not yet been done.</p>

        <p>The raw textual data in PDFs, or in metadata fields, is of course in many cases in Japanese. This was processed
        as found, and stored as utf-8 along with any other text. (Some translation attempts will be discussed in the
        following section).</p>

        <p><a target="_blank" href="https://static.cottagelabs.com/pubman/example.pdf">View a Pubman PDF</a></p>
        <p><a target="_blank" href="https://api.nims.test.cottagelabs.com/pdf?q=_id:1015547">View the imported record</a></p>

        <a name="methods" id="methods"></a>



        <h2>Methods</h2>

        <h3>Full text indexing</h3>

        <p>Hyrax does do fulltext indexing in a SOLR backend, and we had written Hyrax importers for the
        various data, but of course they were suited to the needs of
        the Hyrax system. Also, Hyrax does not include all the TDM tools I wanted to use, and does not yet
        provide an export or full-featured API.<p>

        <p>This along with the XML schemas and dataset structures still being under development
        made it more suitable to import the original data directly, and to put it into a system more suited
        to this investigation. So wrote scripts to convert structured data to JSON, extract text wherever
        possible, and import all data to elasticsearch.</p>

        <p>Elasticsearch uses JSON by default which is very easy to query from javascript in browsers,
        and also in backend code written in node.js or python or many other languages. It automatically
        performs full text indexing, with many features such as stemming and partial matching built in.</p>

        <p><a target="_blank" href="https://www.elastic.co/guide/en/elasticsearch/reference/1.4/index.html">View the Elasticsearch docs</a></p>

        <p>I often use Elasticsearch for data storage and analysis so I already had a cluster running,
        and I also have some TDM software from previous work. The functionality that I have developed
        for these demos is all accessible via an API that I wrote for the purpose (as a plugin to a
        node.js framework I previously wrote), and the root of the API lists all the possible actions:</p>

        <a name="tdm" id="tdm"></a>

        <ul>
          <li><a target="_blank" href="https://api.nims.test.cottagelabs.com">https://api.nims.test.cottagelabs.com</a></li>
        </ul>


        <h3>Text and data mining</h3>

        <p>TDM begins with the simple "mining" processes of digging through everything to find things that might be useful.</p>

        <p>The metadata for the sample datasets can be spread across more than one file, and can contain structured
        metadata in XML but also things like useful comments in CSV files within the ZIPs. So I unpacked and traversed
        all ZIPs, extracted and simplified the XML, and extracted any useuful text from any other file such as CSV, PDF, etc.
        </p>

        <p>For Pubman XML, the three smallest of the four files were processed with simplification to keys:
        properties, relations, resources, components, md-records. This resulted in 1964 records to work with.</p>

        <p>With some raw text to work with, I ran keyword extraction of simple occurrence counts, filtered
        with stopwords, and configured to provide 1 or 2 word n-gram length. I also took the NIMS vocabulary files
        and used them as lookups to automatically produce highly accurate keyword lists for each record.</p>

        <p>I also used the Samurai data to generate researcher name lists and used those as lookups as well.</p>

        <p>Samurai also already includes metadata about the publications of each researcher, which was used for one
        of the demonstrations to follow.</p>

        <p>Using these sorts of lookups we could automatically create metadata for submissions to the Hyrax system,
        and/or provide suggestions to users and curators.</p>

        <p>Within the scope of TDM I also have hamming and levenshtein algorithms available for use in comparing the
        "similarity" of values, or the "distance" between one value and another, which is later used with other techniques.
        Whilst levenshtein is more powerful for identifying items that are similar, hamming is better for working out
        the fewest steps needed to convert one value to another.</p>

        <a name="nlp" id="nlp"></a>
        
        <p>For example the levenshtein difference between 1234567890 and 0123456789 is 2 because they are similar strings, 
        but the hamming distance is 10, because each element of the string needs to change in order to convert one to the other. 
        So for image comparison hamming is more useful, as we want to know how many points need to change to convert one image to another.</p>


        <h3>Natural language processing</h3>

        <p>Building on the simpler TDM, NLP looks not just at the content of text but at the structure. This provides
        abiliites to identify certain types of word such as actions or names, and allows us to do things like search for
        particular kinds of named entities, regardless of how often they appear within a dataset, for example.</p>

        <p>I made use of the University of Cambridge Chemical Tagger text analysis tool,
        although I simplifiedthe results to flatten them to a simpler keyword-style list. It is possible to do more with
        this tool, but unless the source text is restricted to relevant context such as abstracts or methods sections, it
        can have a high false-positive rate.</p>

        <p><a target="_blank" href="http://chemicaltagger.ch.cam.ac.uk">View Cambridge Chemical Tagger website</a></p>

        <p>I then also passed text to an instance of Stanford CoreNLP that I set up for the purpose, and it was used to
        perform entity extraction on each text, generating lists of entities such as persons, locations, organizations,
        which were then stored with the records (and I again used the Samura data to try to filter persons to relevant ones,
        instead of Japanese football players...)</p>

        <p><a target="_blank" href="https://stanfordnlp.github.io/CoreNLP/">View Stanford Core NLP website</a></p>

        <p>I also compared results to the Google natural language entity extraction API, achieving similar results.</p>

        <p>The Google API does have the advantage of automatically including links to Google Knowledge Graph
        and Wikidata where possible, however I was able to replicate this by querying those APIs myself with suitable
        entity search terms, thus building relations between records and wikidata/knowledge entries for entities that could be
        found. This produced preliminary results that would benefit from further work - and would be interesting to link to
        the NIMS wikidata instance once available.</p>

        <a name="language" id="language"></a>

        <p><a target="_blank" href="https://cloud.google.com/natural-language/">View Google natural language API website</a></p>
        <p><a target="_blank" href="https://developers.google.com/knowledge-graph/">View Google knowledge graph API website</a></p>
        <p><a target="_blank" href="https://www.wikidata.org/w/api.php">View Google Wikidata API website</a></p>


        <h3>Language translation</h3>

        <p>I attempted to translate Japanese text to English using the Google natural
        language translation API just to see how it performed, but results were poor, so I deleted the translations
        and disabled this process. Google translate is good for short statements and simple sentences, but large
        chunks of academic text was too difficult.</p>

        <p>Given the language translation issues, before parsing PDFs I first checked the title of the available file
        to see if it ended in .pdf and if it appeared to be in English. To do this I used a language detection library
        which was reliable enough. However some english-title PDFs still mostly/only contain Japanese characters,
        so these were filtered out also. So far I have successfully run 22 PDFs through this process and created records
        for them with a reasonable and understandable amount of text extracted.</p>

        <p>Regarding the future of NLP for Japanese, Stanford CoreNLP does have various language plugins such as
        English and Chinese, but no Japanese yet. Perhaps there is some future work to write a Japanese module!</p>

        <a name="ml" id="ml"></a>

        <p>(Note, of course, that some TDM can be done on Japanese text without translation - but unfortunately I do not
        understand Japanese so I would not have been able to understand my results!)</p>


        <h3>Machine learning</h3>

        <p>A further step from TDM and NLP, ML allows us to "train" an algorithm to identify certain features in a
        dataset. In this case the main algorithm used has been k-means clustering.</p>

        <p>By choosing the number of clusters we would like (I chose six), we can pass datasets to an
        algorithm that begins by randomly matching each record in the dataset to one of the clusters. The
        next step then compares the similarity of the records within each cluster, and redistributes them in
        an attempt to make each cluster more similar. By choosing which value we wish to optimise on, the
        algorithm can "learn" how to cluster datasets on a certain feature.</p>

        <p>For example, any digital image is a dataset - an array of points, each having a set of values
        denoting x, y, red, green, blue (and perhaps alpha). So an image can be submitted to a k-means clustering
        as a dataset consisting of many "records" where each record is a "pixel". This comes in useful in the
        demonstrations to follow.</p>

        <a name="phash" id="phash"></a>

        <p>(There is more potential with ML, particularly in training a neural network to classify submissions
        to certain types, but more examples of real submitted data would be required to train such a system -
        another candidate for future work.)</p>


        <h3>Perceptual hashing</h3>

        <p>To go one more step from just clustering datasets on particular values, or training a network to classify
        submission, we can also use a combination of techniques to do something immediately useful - such as
        identifying similar images.</p>

        <p>A perceptual hash solves the problem of images being of the same thing but having different checksums. Instead
        of matching directly based on the file data, a p-hash picks out key elements of an image (using clustering
        techniques like those described above) and then generates a simple string value that is a hash of the key
        perceptual elements of the image. Any image file that contains the same image, but perhaps shrunk or expanded,
        or rotated, or having altered balance, contrast, etc, should still have a similar p-hash value.</p>

        <p>So whilst a simple checksum could allow us to match identical images without knowing their filenames or
        their accompanying metadata, calculating the hamming distance between two image p-hashes allows us to
        estimate the perceptual similarity of different image files.</p>

        <p>So I extracted data from the Imeji metadata XML files and processed them in the ways described above.
        I then retrieved the images linked in the metadata for each record and calculated a perceptual hash
        for each. One of the demonstrations below shows that this allows us to match similar images based solely on their content.</p>

        <p>(More image analysis processes are also possible, and are mentioned in the demonstration to follow.)</p>



        <a name="demos" id="demos"></a>


        <h2>Demonstrations</h2>

        <a name="dataset" id="dataset"></a>

        <p>Each data type was given a typical faceted search interface demonstrating the ability to extract content from
        various sources and present in a standard way with a search box and dropdowns to filter to specific
        records, showing that we could successfully extract text amd find terms and entities within the source data.
        However the display of results was customised in a range of ways, from simple text records to tiling images and
        interactive graphs.</p>


        <a name="pubman" id="pubman"></a>
        <h3>Pubman XML</h3>

        <p>TODO still in progress</p>


        <h3>Dataset samples</h3>

        <ul>
          <li><a target="_blank" href="https://nims.test.cottagelabs.com/dataset">Dataset search</a></li>
          <li><a target="_blank" href="https://nims.test.cottagelabs.com/dataset/graph">Dataset graphs</a></li>
          <li><a target="_blank" href="https://nims.test.cottagelabs.com/dataset/csv">Dataset CSV charts</a></li>
        </ul>

        <p>Dataset results were displayed first as a normal search results page, with many of the deeply nested values available
        as filters, and then as graphs that can show relations within the data, such as the relation
        between specimens and analyses. Graph parameters can be customised and adjust depending on search criteria.</p>

        <p>Any CSV found in dataset ZIPs were extracted, values mapped onto an x,y plane and visualised as a line graph.
        All data is shown on one chart, then search filters can be used to choose which to display, allowing visual comparison
        of different data sets.</p>

        <a name="imeji" id="imeji"></a>

        <p>For example filtering title to "#3 Ni_3N" which will show two charts that
        map directly onto each other, and the filename filter shows that these are from 2 different files in the same dataset.
        This gives an easy way to verify where result sets are expected to be similar, or to see where they should be
        different but are not.</p>


        <a name="pdf" id="pdf"></a>
        <h3>Pubman PDF</h3>

        <ul>
          <li><a target="_blank" href="https://nims.test.cottagelabs.com/pubman/entities.html">Pubman PDF entity search</a></li>
        </ul>

        <p>TODO still in progress, will show the extracted entities and links out to wikidata etc</p>


        <h3>Imeji</h3>

        <ul>
          <!--<li><a target="_blank" href="https://nims.test.cottagelabs.com/imeji">Imeji search</a></li>-->
          <li><a target="_blank" href="https://nims.test.cottagelabs.com/imeji/tile">Imeji tile search</a></li>
          <li><a target="_blank" href="https://nims.test.cottagelabs.com/imeji/tile?sort=phash">Imeji pHash sort</a></li>
          <li><a target="_blank" href="https://nims.test.cottagelabs.com/imeji/analysis">Imeji analysis</a></li>
          <li><a target="_blank" href="https://nims.test.cottagelabs.com/imeji/analysis?url=http://imeji.nims.go.jp/imeji/file/8/b0/5a/ac/b0-72da-4a56-833a-15c403f1d344/0/thumbnail/b08e0dbe1c3e8dc7bdcdc8ca88623a1f.jpg">Imeji analysis pre-selected example</a></li>
        </ul>

        <p>For Imeji the standard search was customised to show results displayed as image tiles rather than text.
        Scrolling automatically loads more images. Clicking an image shows more metadata, and provides an "analyse image" option.</p>

        <p>Search bar dropdown options also give a unique pHash-sort ability. This displays images next to ones they are similar to,
        demonstrating that we can analyse raw images directly and find similar images in the dataset or in newly uploaded data, using
        perceptual hashes. This is different from just matching identical images, which can be simply done using checksums.</p>

        <p>Selecting image analysis provides a page demonstrating some more extracted image data such as orientation, lightness/darkness,
        average shade, main colours, and a bounding box showing the key area of interest. (Note this does not yet work for the animated images.)</p>

        <p>Average and main colours are calculated using k-means clustering algorithm. The bounding box is calculated by a recursive
        algorithm comparing information about the variation of colours and intensity across the image, seeking the area that stands out the most.</p>


        <a name="samurai" id="samurai"></a>

        <h3>Samurai</h3>

        <ul>
          <!--<li><a target="_blank" href="https://nims.test.cottagelabs.com/samurai">Samurai search</a></li>-->
          <li><a target="_blank" href="https://nims.test.cottagelabs.com/samurai/sankey">Samurai sankey</a></li>
          <li><a target="_blank" href="https://nims.test.cottagelabs.com/samurai/network">Samurai network</a></li>
        </ul>

        <p>This Samurai network graph demonstrates how we can use data visualisation to learn about interesting aspects of data that
        may not be obvious in other ways. By default this interactive network graph shows 100 researchers, linked to the research groups
        they belong to, but also linked to the research groups of every researcher they have co-authored a paper with. This quickly visually
        demonstrates which research groups are the most collaborative, and which researchers have done the most collaborating.</p>

        <p>This kind of interface is probably too complex to be used as a search interface, but it can be used to generate very useful visualisations.
        Hover the mouse pointer over any node to see the name of it, and also to declutter the links to only those related to the node in question.
        Click any node to add it to the search filter, or use the usual filters to do so. For example, filter by a group to see every
        researcher and all the groups that members of the chosen group have collaborated with. The usual search filters also have extra
        buttons to enable you to add that filter type as a node on the graph, and whether to link it or not, and how many of each node to show.</p>

        <p>Note there are 832 researchers in total, it is possible to show them all on screen at once, but the visualisation may become
        slow, depending on the power of your machine. To try this, type options.query.size:832 into the search bar and hit enter...</p>

        <p>TODO try to finish a samurai inverse query vis.</p>

        <a name="search" id="search"></a>

        <p>It is notable how many links there are in this graph. NIMS researchers appear to be highly collaborative between research groups.</p>


        <h3>Advanced search / suggest</h3>

        <a name="future" id="future"></a>

        <p>TODO still in progress</p>



        <h2>Future work</h2>

        <p>We were able to demonstrate text analysis of the available data, as well as showing ways to extract
        value such as keyword analysis and generation, and linking authors to articles, and discovering and
        linking entities between different datasets within the system and also to external datasests.</p>

        <p>But at present it is quite a lot of work to get the data out of the Hyrax system or to get the raw data
        in a suitable state. We could not expect everyone to be willing to make
        such efforts, nor possibly to have the access rights necessary for such detailed work on the raw data.</p>

        <p>So developing a configurable exporter or query API for Hyrax is the most important
        issue to consider next, and would be the best candidate for future work, from the perspective of TDM.</p>

        <p>Here is a list of possible future work - some easy, and can be done with the launch of the new Hyrax
        system, and some pretty complex!</p>

        <ul>
          <li>Implement CSV recommendations to get some more research data to experiment with</li>
          <li>Begin using Hyrax to store more research data, and review later once we see what we have.</li>
          <li>Develop an import process that can analyse records submitted to the Hyrax system
          and extract value to simplify the submission process (autosuggesting keywords, etc)</li>
          <li>Use the processes demonstrated here to further improve the Hyrax user interface, such as suggestion boxes and dropdowns.</li>
          <li>Develop Hyrax so that it is more suited to extracting and/or querying data for TDM by adding an exporter and possibly a custom API</li>
          <li>Develop an additional system, such as that used in this investigation,
          to stand alongside hyrax that can present the exported data for TDM purposes</li>
          <li>Connect Hyrax (via exporter or API) to the NIMS TDM research team tools (whatever they may be - I do not know yet)</li>
          <li>Gain more context and understanding of the data that will be included in real datasets submitted to the system, and use them
          to train more complex machine learning approaches to identify data features (likely to only be possible once the system has been
          running for a while and has more real world data in it).</li>
          <li>Write a Japanese language module for Stanford CoreNLP, or find/develop an improved Japanese translation software (these could be hard).</li>
        </ul>

      </div>
    </div>
  </div>
</div>


<script>
  jQuery(document).ready(function() {

  });
</script>

